{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def set_driver(url):\n",
    "\n",
    "    ser = Service(DRIVER_PATH)\n",
    "    options = webdriver.ChromeOptions()  # Set up Chinese\n",
    "    # options.add_argument('--headless')\n",
    "    options.add_argument('lang=zh_CN.UTF-8')  # Replacement of head\n",
    "    options.add_argument(\n",
    "        'User-Agent=\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36\"')\n",
    "    # options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--headless\")\n",
    "    # PROXY = \"11.456.448.110:8080\"\n",
    "    # options.add_argument('--proxy-server=%s' % PROXY)\n",
    "    options.add_argument(\"--window-size=1920,1080\");\n",
    "    options.add_argument(\"--no-sandbox\");\n",
    "    options.add_argument(\"--disable-extensions\");\n",
    "    options.add_argument(\"--dns-prefetch-disable\");\n",
    "    options.add_argument(\"--disable-gpu\");\n",
    "    # options.setPageLoadStrategy(PageLoadStrategy.NORMAL);\n",
    "    driver = webdriver.Chrome(\n",
    "        service=ser,\n",
    "        options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "    print(\"finish setup driver\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "def init_filename(source, subject):\n",
    "    # /呱吉/PTT.csv\n",
    "    # /呱吉/FB.csv\n",
    "    # ...\n",
    "    folder = f\"/{source}/\"\n",
    "    return subject + folder\n",
    "\n",
    "\n",
    "def write_file():\n",
    "    out_filename = \"\"\n",
    "    with open(out_filename, 'a') as out_file:\n",
    "        for line in in_file:\n",
    "\n",
    "            out_file.write(parsed_line)\n",
    "\n",
    "\n",
    "def init_csv(file_name):\n",
    "\n",
    "    with open(file_name, 'w+') as out_file:\n",
    "        write = csv.writer(out_file)\n",
    "        header = ['Source', 'Title', 'Link', 'Date', 'Summary', 'Text', 'Like']\n",
    "        # PTT, 呱吉吃大便, http://, '2021-11-17', XXXX, XXXXX, 23\n",
    "        write.writerow(header)\n",
    "\n",
    "\n",
    "def string_to_datetime(s):\n",
    "    return datetime.datetime.strptime(s, '%Y/%m/%d')\n",
    "\n",
    "\n",
    "def generate_ytr_name_list(filename=\"Youtuber名單 - 道歉.csv\"):\n",
    "    ytr_list = []\n",
    "    with open(filename, 'r') as f:\n",
    "        rows = csv.reader(f)\n",
    "        for row in rows:\n",
    "            if row[1] == \"\":\n",
    "                pass\n",
    "            if row[1] == \"頻道(官方帳號)\":\n",
    "                continue\n",
    "            ytr_name = []\n",
    "            ytr_name.append(row[1].replace(\n",
    "                \"/\", \" \").replace(\"   \", \" \").split(\"\\n\")[0])\n",
    "\n",
    "            ytr_name.extend(row[3].replace(\"、\", \"\\n\").split(\"\\n\"))\n",
    "            ytr_list.append(list(dict.fromkeys(ytr_name)))\n",
    "    return ytr_list\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "DRIVER_PATH = '/home/bobo/Desktop/bobo/ML_final/bobo/crawlers/chromedriver'\n",
    "# DRIVER_PATH = \"/Users/bobo/OneDrive - 台灣微軟/0. bobo/Codes/stratechery crawler/chromedriver\"\n",
    "apple_news_url = \"https://tw.appledaily.com/search/阿神/\"\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def scroll_to_button(driver):\n",
    "    reached_page_end = False\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while not reached_page_end:\n",
    "        driver.find_element(By.XPATH, '//body').send_keys(Keys.END)\n",
    "        time.sleep(5)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if last_height == new_height:\n",
    "            reached_page_end = True\n",
    "        else:\n",
    "            last_height = new_height\n",
    "    print(\"reach end\")\n",
    "    # driver.quit()\n",
    "\n",
    "\n",
    "def crawl_page(driver, filename):\n",
    "    contianer_list = driver.find_element(\n",
    "        By.CLASS_NAME, \"article-container\").find_elements(By.CLASS_NAME, \"flex-feature\")\n",
    "    \n",
    "    if contianer_list == []:\n",
    "        print(contianer_list)\n",
    "        print(\"is empty\")\n",
    "        return\n",
    "\n",
    "\n",
    "    print(\"len: \", len(contianer_list))\n",
    "\n",
    "    filename = filename\n",
    "    with open(filename, 'a+') as f:\n",
    "        write = csv.writer(f)\n",
    "\n",
    "        header = ['Source', 'Title', 'Link', 'Date', 'Summary', 'Text', 'Like']\n",
    "        dic = {i: \"\" for i in header}\n",
    "\n",
    "        for i in contianer_list:\n",
    "            dic['Source'] = \"apple_news\"\n",
    "            dic['Title'] = i.find_element(\n",
    "                By.CLASS_NAME, 'storycard-headline').text.replace(\"  \", \" \")\n",
    "            dic['Link'] = i.find_element(\n",
    "                By.TAG_NAME, 'a').get_attribute(\"href\")\n",
    "            dic['Date'] = i.find_element(By.CLASS_NAME, 'timestamp').text\n",
    "            dic['Summary'] = \"\"\n",
    "            dic['Text'] = \"\"\n",
    "            dic['Like'] = 0\n",
    "\n",
    "            line = dic.values()\n",
    "            write.writerow(line)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# tag search\n",
    "def crawl_apple_news(path, tag_url):\n",
    "    driver = set_driver(tag_url)\n",
    "\n",
    "    scroll_to_button(driver)\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    crawl_page(driver, filename=path+\"apple_news.csv\")\n",
    "\n",
    "    driver.quit()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ytr_list = generate_ytr_name_list(\"Youtuber名單 - 工作表3.csv\")\n",
    "ytr_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['阿滴英文', '阿滴', '都省瑞'],\n",
       " ['啾啾鞋'],\n",
       " ['亮生活 / Bright Side', '亮生活'],\n",
       " ['理科太太', '陳映彤'],\n",
       " ['Taiwan Bar', '台灣吧'],\n",
       " ['那些電影教我的事 Lessons from Movies'],\n",
       " ['志祺七七 X 圖文不符', '志祺'],\n",
       " ['柴鼠兄弟 ZRBros', '柴鼠兄弟'],\n",
       " ['許伯&簡芝—倉鼠人', '許伯&簡芝－倉鼠人', '許伯', '簡芝', '倉鼠人'],\n",
       " ['Cheap', 'cheap'],\n",
       " ['窮奢極慾'],\n",
       " ['三個字SunGuts'],\n",
       " ['維思維WeisWay', '維思維', 'WeisWay'],\n",
       " ['營養健身葛格Peeta', 'Peeta', '葛格'],\n",
       " ['蒼藍鴿的醫學天地', '蒼藍鴿', '蒼藍教主'],\n",
       " ['SKIMMY 你的網路閨蜜', 'skimmy你的網路閨密', 'skimmy'],\n",
       " ['劉芒'],\n",
       " ['Dr. Ivan 6', 'Dr.Ivan'],\n",
       " ['冏冏電台', '冏冏子', '冏星人', '冏冏'],\n",
       " ['超級歪SuperY', '超級歪'],\n",
       " ['快樂大學Happiness University', '快樂大學'],\n",
       " ['昆蟲擾西吳沁婕Dee the bugbuff', '昆蟲擾西吳沁婕', '昆蟲擾西', '吳沁婕'],\n",
       " ['好味營養師品瑄', '好味營養師', '品瑄', '品瑄營養師'],\n",
       " ['飽妮'],\n",
       " ['啟點文化'],\n",
       " ['英文易開罐'],\n",
       " ['講日文的台灣女生 Tiffany', '講日文的台灣女生Tiffany', '講日文的台灣女生'],\n",
       " ['胃酸人 위산맨', '胃酸人'],\n",
       " ['GARY G腿講NBA故事'],\n",
       " ['Onion Man', '洋蔥'],\n",
       " ['蛋哥超有事', '蛋哥'],\n",
       " ['微疼', '我得了一種吃菜就會死的病-微疼'],\n",
       " ['阿啾小劇場', '阿啾', '阿啾繪圖同萌'],\n",
       " ['鹿人與泥鰍小劇場', '鹿人', '泥鰍'],\n",
       " ['啾啾妹'],\n",
       " ['霸軒與小美 Baxuan & Mei', '霸軒'],\n",
       " ['DeluCat迪鹿', '迪鹿'],\n",
       " ['BugCat Capoo', 'BugCat', 'Capoo', '貓貓蟲咖波'],\n",
       " ['囂搞', 'Shaogao'],\n",
       " ['CFABC 鵝肉麵', '鹽酥的鵝肉麵'],\n",
       " ['蓋彼 Gabi SDK', '蓋彼', 'Gabi'],\n",
       " ['玩什麼鬼啦'],\n",
       " ['這群人TGOP', '這群人', '群人', 'This Group Of People', 'TGOP'],\n",
       " ['葉式特工 Yes Ranger', '葉式特工'],\n",
       " ['木曜4超玩'],\n",
       " ['眾量級CROWD', '重量級', '眾量級CROWDxFUN生活'],\n",
       " ['黃氏兄弟'],\n",
       " ['WACKYBOYS 反骨男孩', 'WACKYBOYS', '反骨男孩', '反骨'],\n",
       " ['白癡公主', '戴平雅', 'A減', '痴痴', '王美', '貧乳界女神'],\n",
       " ['聖結石Saint', '聖結石', '曾聖傑'],\n",
       " ['Sandy&Mandy', 'Sandy and Mandy'],\n",
       " ['放火 Louis', '放火', '李育群'],\n",
       " ['NyoNyoTV妞妞TV', 'NyoNyoTV', '妞妞'],\n",
       " ['鍾明軒', '煎熬弟'],\n",
       " [\"蔡桃貴 蔡阿嘎二伯's Family\", '蔡桃貴', '蔡阿嘎', '二伯'],\n",
       " ['導演好了沒', 'Is The Director Ready?'],\n",
       " ['黃大謙', '大謙'],\n",
       " ['上班不要看 NSFW', '上班不要看', '上班可以看'],\n",
       " ['肌肉山山jiroushanshan', '肌肉山山', 'jiroushanshan', '山山'],\n",
       " ['反正我很閒'],\n",
       " ['瑀熙 Yuci', '瑀熙', 'Yuci', '大王瑀康熙'],\n",
       " ['Stand up, Brian! 博恩站起來！', '博恩', '鋼鋼'],\n",
       " ['六指淵 Huber', '六指淵'],\n",
       " ['最近紅什麼'],\n",
       " ['大麻煩'],\n",
       " ['阿翰po影片', '阿翰'],\n",
       " ['綠眼鏡Punk', '綠眼鏡', '徐晢軒'],\n",
       " [''],\n",
       " [''],\n",
       " ['阿神', '芋圓柚子OEUR.studio'],\n",
       " ['Joeman', '九妹'],\n",
       " ['谷阿莫'],\n",
       " ['滴妹', '都冠伶'],\n",
       " ['HowFun', 'HowHow', 'How哥'],\n",
       " ['愛莉莎莎 Alisasa', '愛莉莎莎'],\n",
       " ['見習網美小吳'],\n",
       " ['米砂Misa', '米砂', 'Misa', '老濕姬', '江佳儒'],\n",
       " ['波特王 Potter King', '波特王', 'Potter King'],\n",
       " ['丁特dinter', '丁特', 'dinter', '薛弘偉'],\n",
       " ['一隻阿圓 I am CIRCLE', '一隻阿圓', '阿圓'],\n",
       " ['Toyz'],\n",
       " ['呱吉'],\n",
       " ['龍龍LungLung', '龍龍', 'LungLung'],\n",
       " ['許藍方博士 Dr. Gracie', '許藍方博士', 'Dr. Gracie', '許藍方'],\n",
       " ['孫安佐', 'Edward Sun', '孫華'],\n",
       " ['VITO維特', '維特'],\n",
       " ['NanaQ'],\n",
       " ['統神大戲院', '統神', '張嘉航'],\n",
       " [''],\n",
       " ['千千進食中', '千千', 'Chien-Chien'],\n",
       " ['MASAの料理ABC', 'MASA'],\n",
       " ['蘿潔塔的廚房'],\n",
       " ['大蛇丸'],\n",
       " ['古娃娃WawaKu', '古娃娃', 'wawawaku'],\n",
       " ['詹姆士姆士流官方專屬頻道', '詹姆士'],\n",
       " ['台灣大胃王丁丁/Ding-Ding', '台灣大胃王丁丁', 'Ding-Ding'],\n",
       " ['Dream Chef Home 夢幻廚房在我家', 'Dream Chef Home', '夢幻廚房在我家'],\n",
       " ['路路 LULU', '路路', 'LULU'],\n",
       " ['RICO'],\n",
       " ['糖餃子Sweet Dumpling', '糖餃子', 'Sweet Dumpling'],\n",
       " ['菜單研究所'],\n",
       " ['克里斯餐桌'],\n",
       " ['黃聖凱', '錵鑶居家料理小教室'],\n",
       " ['明聰'],\n",
       " ['吃貨豪豪 HowHowEat', '吃貨豪豪', 'HowHowEat'],\n",
       " ['巧兒灶咖 Ciao! Kitchen', '巧兒灶咖', 'Ciao! Kitchen'],\n",
       " ['厭世甜點店', '拿拿摳'],\n",
       " ['Brian Cuisine Inc', '不萊嗯的烘焙廚房'],\n",
       " ['找蔬食Traveggo', '找蔬食', 'Traveggo'],\n",
       " ['簡單哥'],\n",
       " ['索艾克'],\n",
       " ['特盛吃貨艾嘉Abby Big Eater', '特盛吃貨艾嘉', 'Abby Big Eater'],\n",
       " ['小田太太の玩樂廚房', '小田太太'],\n",
       " ['黃阿瑪的後宮生活', '黃阿瑪', '志銘與狸貓 '],\n",
       " ['豆漿- SoybeanMilk', '豆漿-SoybeanMilk', '漿爸'],\n",
       " ['好味小姐 Lady Flavor', '好味小姐', 'Lady Flavor'],\n",
       " ['希露弟弟啃雞腿'],\n",
       " ['拉姆有幾噗'],\n",
       " ['貓咪&狗狗搞笑 Cat&dog Funny videos'],\n",
       " ['柴犬Nana和阿楞的一天'],\n",
       " ['連環泡有芒果影片專區'],\n",
       " ['AC草影水族'],\n",
       " ['南瓜 Pumpkin'],\n",
       " ['J爸'],\n",
       " ['阿坤的水族遊樂園'],\n",
       " ['臺灣蟻窟AntsFormosa', '臺灣蟻窟', 'AntsFormosa'],\n",
       " ['Hello Catie'],\n",
       " [\"I'm Charlie\", 'Charlene', '我是查理'],\n",
       " ['黃小米 Mii', '黃小米', 'Mii'],\n",
       " ['Meg Lu'],\n",
       " ['It‘s Jcnana 蒨蒨', 'It‘s Jcnana', '蒨蒨'],\n",
       " ['丹妮婊姐星球', '丹妮婊姐', '婊姐'],\n",
       " ['居妮Ginny Daily', '居妮', 'Ginny Daily'],\n",
       " ['Astor'],\n",
       " ['SelinaCheng Life', '鄭雅云'],\n",
       " ['heyitsmindy', 'Mindy'],\n",
       " ['Celeste Wu 大沛', 'Celeste Wu', '大沛'],\n",
       " ['Jessica Lin'],\n",
       " ['Tang_ful'],\n",
       " ['Kimi紀卜心', '紀卜心'],\n",
       " [''],\n",
       " [''],\n",
       " ['Gary蓋瑞', '蓋瑞薯淑'],\n",
       " ['Dinter', '丁特'],\n",
       " ['Toyz', '劉偉健', 'Kurtis Lau Wai-kin'],\n",
       " ['Jinnytty 企鵝妹 윰찌 ', '企鵝妹', 'Jinny', '柳允進'],\n",
       " ['鼻地大師國動-張葦航', '國動', '張葦航', '動主播', '鼻地大師', '國棟']]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "base_path = \"data/\"\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "for ytr_name in ytr_list:\n",
    "    \n",
    "    if ytr_name == ['']:\n",
    "        continue\n",
    "        \n",
    "    print(ytr_name)\n",
    "    folder_path = base_path + ytr_name[0] + \"/\"\n",
    "    \n",
    "    # if folder not exists, create\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)    \n",
    "    \n",
    "    # if file exists, continue, else, create\n",
    "    if os.path.exists(folder_path+\"apple_news.csv\"):\n",
    "        pass\n",
    "    else:\n",
    "        init_csv(folder_path+\"apple_news.csv\")\n",
    "        for name in ytr_name:\n",
    "            apple_url = \"https://tw.appledaily.com/search/\"+name+\"/\"\n",
    "            try:\n",
    "                crawl_apple_news(folder_path, apple_url)\n",
    "            except Exception as e:\n",
    "                print(\"error :\", e)\n",
    "                continue    \n",
    "        \n",
    "    print(\"*\"*20)\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ytr_list' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a2270e940df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mytr_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mytr_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mytr_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ytr_list' is not defined"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## crawl web text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_filename_from_folder(folder_path, ext=\"\"):\n",
    "    file_name_list = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        name = os.path.join(folder_path + \"/\"+ file_name)\n",
    "        \n",
    "        # get file extension\n",
    "        file_ext = os.path.splitext(file_name)[1]\n",
    "        # if user give ext and the file extension is not the same, pass\n",
    "        if ext and file_ext != ext:\n",
    "            continue\n",
    "            \n",
    "        file_name_list.append(name)\n",
    "        \n",
    "    return sorted(file_name_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "driver = set_driver(apple_news_url)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finish setup driver\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print apple news which hasn't finish crawling\n",
    "for idx, folder_path in enumerate(get_filename_from_folder(\"./data\")):\n",
    "    try:\n",
    "        for file_path in get_filename_from_folder(folder_path, \".csv\"):\n",
    "                    # if not apple news, continue\n",
    "            if not file_path.split(\"/\")[-1] == \"apple_news.csv\":\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.shape[0] < 2 or df[\"Text\"].notnull().sum() > 2:\n",
    "                continue\n",
    "            else:\n",
    "                print(file_path)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# crawl index\n",
    "for idx, folder_path in enumerate(get_filename_from_folder(\"./data\")):\n",
    "    for file_path in get_filename_from_folder(folder_path, \".csv\"):\n",
    "        # if not apple news, continue\n",
    "        if not file_path.split(\"/\")[-1] == \"apple_news.csv\":\n",
    "            continue\n",
    "        \n",
    "        print(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # if already have data, continue\n",
    "        if df.shape[0] < 2 or df[\"Text\"].notnull().sum() > 1:\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "            \n",
    "        for row, link in enumerate(df.loc[:, \"Link\"]):\n",
    "            print(row, link)\n",
    "            driver.get(link)\n",
    "            time.sleep(1)\n",
    "            # soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            \n",
    "            text = driver.find_element(By.ID, \"articleBody\").text\n",
    "            \n",
    "            df.loc[row,\"Text\"] = text\n",
    "            \n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(\"*\"*20)\n",
    "        time.sleep(5)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "# fix date\n",
    "for idx, folder_path in enumerate(get_filename_from_folder(\"./data\")):\n",
    "    try:\n",
    "        for file_path in get_filename_from_folder(folder_path, \".csv\"):\n",
    "            # if not apple news, continue\n",
    "            if not file_path.split(\"/\")[-1] == \"apple_news.csv\":\n",
    "                continue\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            for idx, row in enumerate(df['Date']):\n",
    "                date = row.split(\" \")\n",
    "\n",
    "                if date[0] == \"出版時間:\":\n",
    "                    df.loc[idx, \"Date\"] = date[1]\n",
    "            df.to_csv(file_path, index=False)\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error:  [Errno 20] Not a directory: './data/Untitled.ipynb'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#transform apple data to right format\n",
    "for idx, folder_path in enumerate(get_filename_from_folder(\"./data\")):\n",
    "    try:\n",
    "        for file_path in get_filename_from_folder(folder_path, \".csv\"):\n",
    "            # if not apple news, continue\n",
    "            if not file_path.split(\"/\")[-1] == \"apple_news.csv\":\n",
    "                continue\n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.shape[1] >= 8:\n",
    "\n",
    "                print(file_path)\n",
    "                print(df.shape)\n",
    "                df = df[['Source', 'Title', 'Link', 'Date', 'Summary', 'Text',\n",
    "                         'Like']]\n",
    "                df.to_csv(file_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./data/Hello Catie/apple_news.csv\n",
      "(2, 10)\n",
      "error:  [Errno 20] Not a directory: './data/Untitled.ipynb'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#remove duplicate news\n",
    "for idx, folder_path in enumerate(get_filename_from_folder(\"./data\")):\n",
    "    try:\n",
    "        for file_path in get_filename_from_folder(folder_path, \".csv\"):\n",
    "            # if not apple news, continue\n",
    "            if not file_path.split(\"/\")[-1] == \"apple_news.csv\":\n",
    "                continue\n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.shape != df.drop_duplicates().shape:\n",
    "                print(file_path)\n",
    "                print(df.shape, df.drop_duplicates().shape)\n",
    "                \n",
    "                df = df.drop_duplicates()\n",
    "            df.to_csv(file_path, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error: \", e)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error:  [Errno 20] Not a directory: './data/Untitled.ipynb'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# fix singel file\n",
    "df = pd.read_csv(\n",
    "    \"/Users/bobo/OneDrive - 台灣微軟/0. bobo/Codes/crawlers/data/NanaQ/apple_news.csv\")\n",
    "if df.shape[1] >= 8:\n",
    "\n",
    "    print(df.shape)\n",
    "    df = df[['Source', 'Title', 'Link', 'Date', 'Summary', 'Text',\n",
    "                'Like']]\n",
    "    df.to_csv(\n",
    "        \"/Users/bobo/OneDrive - 台灣微軟/0. bobo/Codes/crawlers/data/NanaQ/apple_news.csv\", index=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 10)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'file_path' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6d20064ddda0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     df = df[['Source', 'Title', 'Link', 'Date', 'Summary', 'Text',\n\u001b[1;32m      7\u001b[0m                 'Like']]\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d35b18824a72466740eca9854c21d0838d2a95cb0ef047a5bc842bf6a70449"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}